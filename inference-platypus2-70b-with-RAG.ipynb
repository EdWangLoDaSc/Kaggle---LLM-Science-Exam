{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Platypus2-70B + Wikipedia RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# installing offline dependencies\n",
    "!cp -rf /kaggle/input/sentence-transformers-222/sentence-transformers /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/working/sentence-transformers\n",
    "!pip install -U /kaggle/input/blingfire-018/blingfire-0.1.8-py3-none-any.whl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-10-11T04:01:16.648732Z",
     "iopub.status.busy": "2023-10-11T04:01:16.648372Z",
     "iopub.status.idle": "2023-10-11T04:03:14.966657Z",
     "shell.execute_reply": "2023-10-11T04:03:14.965514Z",
     "shell.execute_reply.started": "2023-10-11T04:01:16.648701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Installing offline dependencies\n",
    "!pip install -U --no-deps /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -U --no-deps /kaggle/input/datasets-214/datasets-2.14.5-py3-none-any.whl\n",
    "!pip install -U --no-deps /kaggle/input/optimum-113/optimum-1.13.2-py3-none-any.whl\n",
    "!pip install -U --no-deps /kaggle/input/transformers-432/transformers-4.32.1-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T04:03:14.96923Z",
     "iopub.status.busy": "2023-10-11T04:03:14.968947Z",
     "iopub.status.idle": "2023-10-11T04:03:37.547426Z",
     "shell.execute_reply": "2023-10-11T04:03:37.546422Z",
     "shell.execute_reply.started": "2023-10-11T04:03:14.969198Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import logging\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Condition\n",
    "import ctypes\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# For RAG\n",
    "import faiss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_from_disk, Dataset\n",
    "\n",
    "NUM_TITLES = 5\n",
    "MAX_SEQ_LEN = 512\n",
    "MODEL_PATH = \"/kaggle/input/bge-small-faiss/\"\n",
    "\n",
    "# For LLM\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils.modeling import set_module_tensor_to_device\n",
    "from safetensors.torch import load_file\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "\n",
    "N_BATCHES = 5\n",
    "MAX_LENGTH = 4096\n",
    "MAX_CONTEXT = 1200\n",
    "# With NUM_TITLES = 5, the median lenght of a context if 1100 tokens (Q1: 900, Q3: 1400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T04:03:37.549174Z",
     "iopub.status.busy": "2023-10-11T04:03:37.548847Z",
     "iopub.status.idle": "2023-10-11T04:03:37.612684Z",
     "shell.execute_reply": "2023-10-11T04:03:37.611796Z",
     "shell.execute_reply.started": "2023-10-11T04:03:37.549139Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to clean RAM & vRAM\n",
    "def clean_memory():\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile parsed_context_search.py\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import blingfire as bf\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "import faiss\n",
    "from faiss import write_index, read_index, read_VectorTransform\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "\n",
    "class CFG:\n",
    "    EMB_MODEL = \"/kaggle/input/sentencetransformer-hubs/gte-base\"\n",
    "    INDEX_PATH = \"/kaggle/input/wikipedia-stem-index/parsed_gte-base.index\"\n",
    "    WIKI_PLAINTEXT_PATH = \"/kaggle/input/wikipedia-stem-plaintext/parsed.parquet\"\n",
    "    \n",
    "    MAX_LENGTH = 512\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_DOC_NUM = 5\n",
    "    \n",
    "    DEBUG = False\n",
    "\n",
    "trn = pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n",
    "\n",
    "## Combine all answers\n",
    "trn = trn.fillna('None')\n",
    "trn['answer_all'] = trn.apply(lambda x: \" \".join([str(x['A']), str(x['B']), str(x['C']), str(x['D']), str(x['E'])]), axis=1)\n",
    "## Search using the prompt and answers to guide the search\n",
    "trn['prompt_answer_stem'] = trn['prompt'] + \" \" +trn['prompt'] + \" \" +trn['prompt'] + \" \" + trn['answer_all']\n",
    "\n",
    "model = SentenceTransformer(CFG.EMB_MODEL, device='cuda')\n",
    "model.max_seq_length = CFG.MAX_LENGTH\n",
    "\n",
    "prompt_embeddings = model.encode(trn.prompt_answer_stem.values, batch_size=CFG.BATCH_SIZE, device=0, show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\n",
    "prompt_embeddings = prompt_embeddings.detach().cpu().numpy()\n",
    "\n",
    "res = faiss.StandardGpuResources()\n",
    "sentence_index = read_index(CFG.INDEX_PATH)\n",
    "sentence_index_gpu = faiss.index_cpu_to_gpu(res, 0, sentence_index)\n",
    "\n",
    "QUERY_SIZE = 8\n",
    "search_score = []\n",
    "search_index = []\n",
    "total = prompt_embeddings.shape[0]\n",
    "for i in tqdm(range(0, total, QUERY_SIZE)):\n",
    "    ss, si = sentence_index_gpu.search(prompt_embeddings[i:i+QUERY_SIZE], CFG.MAX_DOC_NUM)\n",
    "    search_score.append(ss)\n",
    "    search_index.append(si)\n",
    "\n",
    "search_score = np.concatenate(search_score)\n",
    "search_index = np.concatenate(search_index)\n",
    "\n",
    "## Save memory - delete sentence_index since it is no longer necessary\n",
    "del sentence_index\n",
    "del sentence_index_gpu\n",
    "del prompt_embeddings\n",
    "#del model\n",
    "_ = gc.collect()\n",
    "#torch.cuda.empty_cache()\n",
    "libc.malloc_trim(0)\n",
    "\n",
    "df = pd.read_parquet(CFG.WIKI_PLAINTEXT_PATH,\n",
    "                     columns=['text'])\n",
    "\n",
    "contexts = []\n",
    "\n",
    "for i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n",
    "    context = \"\"\n",
    "    scr_idx = idx\n",
    "    context_list = df.loc[scr_idx].text.tolist()\n",
    "    context += \" \".join(context_list)\n",
    "    contexts.append(context)\n",
    "\n",
    "## Save memory - delete df since it is no longer necessary\n",
    "del df\n",
    "_ = gc.collect()\n",
    "libc.malloc_trim(0)\n",
    "\n",
    "trn['context'] = contexts\n",
    "\n",
    "save_cols = ['id', \"prompt\", \"context\",\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "trn[save_cols].to_csv(\"./test_context.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python parsed_context_search.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./test_context.csv\", index_col=\"id\")\n",
    "\n",
    "# Variable used to avoid running the notebook for 3 hours when submitting. Credit : CPMP\n",
    "IS_TEST_SET = len(df) != 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Wikipedia Retrieval Augmented Generation (RAG)\n",
    "\n",
    "The following code is adapted from [the notebook of @MGöksu](https://www.kaggle.com/code/mgoksu/0-807-sharing-my-trained-with-context-model) and [the notebook of @MB](https://www.kaggle.com/code/mbanaei/86-2-with-only-270k-articles/notebook). We use the [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) to embed the Wikipedia dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T04:03:37.615707Z",
     "iopub.status.busy": "2023-10-11T04:03:37.615141Z",
     "iopub.status.idle": "2023-10-11T04:03:37.624334Z",
     "shell.execute_reply": "2023-10-11T04:03:37.623587Z",
     "shell.execute_reply.started": "2023-10-11T04:03:37.615674Z"
    }
   },
   "outputs": [],
   "source": [
    "# New SentenceTransformer class similar to the one used in @Mgöksu notebook but relying on the transformers library only\n",
    "\n",
    "class SentenceTransformer:\n",
    "    def __init__(self, checkpoint, device=\"cuda:0\"):\n",
    "        self.device = device\n",
    "        self.checkpoint = checkpoint\n",
    "        self.model = AutoModel.from_pretrained(checkpoint).to(self.device).half()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    def transform(self, batch):\n",
    "        tokens = self.tokenizer(batch[\"text\"], truncation=True, padding=True, return_tensors=\"pt\", max_length=MAX_SEQ_LEN)\n",
    "        return tokens.to(self.device)  \n",
    "\n",
    "    def get_dataloader(self, sentences, batch_size=32):\n",
    "        sentences = [\"Represent this sentence for searching relevant passages: \" + x for x in sentences]\n",
    "        dataset = Dataset.from_dict({\"text\": sentences})\n",
    "        dataset.set_transform(self.transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        return dataloader\n",
    "\n",
    "    def encode(self, sentences, show_progress_bar=False, batch_size=32):\n",
    "        dataloader = self.get_dataloader(sentences, batch_size=batch_size)\n",
    "        pbar = tqdm(dataloader) if show_progress_bar else dataloader\n",
    "\n",
    "        embeddings = []\n",
    "        for batch in pbar:\n",
    "            with torch.no_grad():\n",
    "                e = self.model(**batch).pooler_output\n",
    "                e = F.normalize(e, p=2, dim=1)\n",
    "                embeddings.append(e.detach().cpu().numpy())\n",
    "        embeddings = np.concatenate(embeddings, axis=0)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Run Platypus2-70B\n",
    "\n",
    "To run such a large model on a single T4 GPU, we run it layer by layer and sample by sample. The model below has been finetuned by the SUStech team, for a zero-shot version refer to Version 15 of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T04:04:37.395265Z",
     "iopub.status.busy": "2023-10-11T04:04:37.394613Z",
     "iopub.status.idle": "2023-10-11T04:04:37.458622Z",
     "shell.execute_reply": "2023-10-11T04:04:37.457721Z",
     "shell.execute_reply.started": "2023-10-11T04:04:37.395224Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create symlinks from kaggle datasets to fake cached model\n",
    "\n",
    "checkpoint_path = Path(\"/root/.cache/\")\n",
    "checkpoint_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "for part in [1, 2, 3]:\n",
    "    source_dir = Path(f'/kaggle/input/platypus2-chuhac2-part{part}')\n",
    "    for path in source_dir.glob(\"*\"):\n",
    "        (checkpoint_path / path.name).symlink_to(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T04:04:37.465027Z",
     "iopub.status.busy": "2023-10-11T04:04:37.462861Z",
     "iopub.status.idle": "2023-10-11T04:04:37.478887Z",
     "shell.execute_reply": "2023-10-11T04:04:37.476443Z",
     "shell.execute_reply.started": "2023-10-11T04:04:37.464991Z"
    }
   },
   "outputs": [],
   "source": [
    "class WeightsLoader:\n",
    "    \"\"\"\n",
    "    Thread-safe class to load the weights of the model.\n",
    "    The weights are loaded in the background and can be accessed with get_state_dict().\n",
    "    All devices must call set_state_dict() before the weights are loaded.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_path, devices):\n",
    "        self.checkpoint_path = Path(checkpoint_path)\n",
    "        self.states = {device: None for device in devices}\n",
    "        self.state_dict = None\n",
    "        self.condition = Condition()\n",
    "        \n",
    "    def get_state_dict(self, device):\n",
    "        with self.condition:\n",
    "            while self.states[device] is not None:\n",
    "                self.condition.wait()\n",
    "            \n",
    "            result = self.state_dict\n",
    "            self.states[device] = None\n",
    "            \n",
    "            if not any(self.states.values()):\n",
    "                self.condition.notify_all()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def set_state_dict(self, layer_name, device):\n",
    "        with self.condition:\n",
    "            self.states[device] = layer_name\n",
    "            if all(self.states.values()):\n",
    "                assert len(set(self.states.values())) == 1, \"All devices should load the same layer\"\n",
    "                self.state_dict = load_file(self.checkpoint_path / (layer_name + \".safetensors\"), device=\"cpu\")\n",
    "                for d in self.states:\n",
    "                    self.states[d] = None\n",
    "                self.condition.notify_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T04:04:37.480761Z",
     "iopub.status.busy": "2023-10-11T04:04:37.480055Z",
     "iopub.status.idle": "2023-10-11T04:04:37.513682Z",
     "shell.execute_reply": "2023-10-11T04:04:37.512512Z",
     "shell.execute_reply.started": "2023-10-11T04:04:37.48073Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class for sharded llama\n",
    "class ShardedLlama:\n",
    "    def __init__(self, checkpoint_path, weights_loader, device=\"cuda:0\", dtype=torch.float16):\n",
    "        \"\"\"\n",
    "        Sharded version of LlamaForCausalLM : the model is splitted into layer shards to reduce GPU memory usage.\n",
    "        During the forward pass, the inputs are processed layer by layer, and the GPU memory is freed after each layer.\n",
    "        To avoid loading the layers multiple times, we could save all the intermediate activations in RAM, but\n",
    "        as Kaggle accelerators have more GPU memory than CPU, we simply batch the inputs and keep them on the GPU.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        checkpoint_path : str or Path\n",
    "            path to the checkpoint\n",
    "        weights_loader : WeightsLoader\n",
    "            object to load the weights\n",
    "        device : str, optional\n",
    "            device, by default \"cuda:0\"\n",
    "        dtype : torch.dtype, optional\n",
    "            dtype, by default torch.float16\n",
    "        \"\"\"\n",
    "        \n",
    "        # Save parameters\n",
    "        self.checkpoint_path = Path(checkpoint_path)\n",
    "        self.weights_loader = weights_loader\n",
    "        self.device = device \n",
    "        self.dtype = dtype\n",
    "\n",
    "        # Create model\n",
    "        self.config = AutoConfig.from_pretrained(self.checkpoint_path)   \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        self.init_model()\n",
    "        self.layer_names = [\"model.embed_tokens\"] + [f\"model.layers.{i}\" for i in range(len(self.model.model.layers))] + [\"model.norm\", \"value_head\"]\n",
    "\n",
    "    def init_model(self):\n",
    "    \n",
    "        # Load meta model (no memory used)\n",
    "        with init_empty_weights():\n",
    "            self.model = AutoModelForCausalLM.from_config(self.config)\n",
    "            self.model.lm_head = torch.nn.Linear(8192, 8, bias=False) # originally 32k\n",
    "            self.model.eval()\n",
    "            self.model = BetterTransformer.transform(self.model) # enable flash attention\n",
    "            self.model.tie_weights()\n",
    "            \n",
    "        self.layers = [self.model.model.embed_tokens] + list(self.model.model.layers) + [self.model.model.norm, self.model.lm_head]\n",
    "\n",
    "        # Move buffers to device (not that much GPU memory used)\n",
    "        for buffer_name, buffer in self.model.named_buffers():\n",
    "            set_module_tensor_to_device(self.model, buffer_name, self.device, value=buffer, dtype=self.dtype)\n",
    "\n",
    "    def load_layer_to_cpu(self, layer_name):\n",
    "        self.weights_loader.set_state_dict(layer_name, self.device)\n",
    "        state_dict = self.weights_loader.get_state_dict(self.device)\n",
    "        if \"value_head.weight\" in state_dict:\n",
    "            state_dict = {\"lm_head.weight\" : state_dict[\"value_head.weight\"]}\n",
    "        return state_dict\n",
    "        \n",
    "    def move_layer_to_device(self, state_dict):\n",
    "        for param_name, param in state_dict.items():\n",
    "            assert param.dtype != torch.int8, \"int8 not supported (need to add fp16_statistics)\"\n",
    "            set_module_tensor_to_device(self.model, param_name, self.device, value=param, dtype=self.dtype)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        # inputs = [(prefix, suffix), ...] with prefix.shape[0] = 1 and suffix.shape[0] = 5\n",
    "        \n",
    "        # Reboot the model to make sure buffers are loaded and memory is clean\n",
    "        del self.model\n",
    "        clean_memory()\n",
    "        self.init_model()\n",
    "        \n",
    "       # Send batch to device\n",
    "        batch = [(prefix.to(self.device), suffix.to(self.device)) for prefix, suffix in inputs]\n",
    "        n_suffixes = len(batch[0][1])\n",
    "        suffix_eos = [(suffix != self.tokenizer.pad_token_id).sum(1) - 1 for _, suffix in inputs]\n",
    "\n",
    "        # Create attention mask for the largest input, and position ids to use KV cache\n",
    "        attention_mask = torch.ones(MAX_LENGTH, MAX_LENGTH)\n",
    "        attention_mask = attention_mask.triu(diagonal=1)[None, None, ...] == 0\n",
    "        attention_mask = attention_mask.to(self.device)\n",
    "        position_ids = torch.arange(MAX_LENGTH, dtype=torch.long, device=self.device)[None, :]\n",
    "\n",
    "        with ThreadPoolExecutor() as executor, torch.inference_mode():\n",
    "\n",
    "            # Load first layer\n",
    "            future = executor.submit(self.load_layer_to_cpu, \"model.embed_tokens\")\n",
    "\n",
    "            for i, (layer_name, layer) in tqdm(enumerate(zip(self.layer_names, self.layers)), desc=self.device, total=len(self.layers)):\n",
    "\n",
    "                # Load current layer and prepare next layer\n",
    "                state_dict = future.result()\n",
    "                if (i + 1) < len(self.layer_names):\n",
    "                    future = executor.submit(self.load_layer_to_cpu, self.layer_names[i + 1])\n",
    "                self.move_layer_to_device(state_dict)\n",
    "                \n",
    "                # Run layer\n",
    "                for j, (prefix, suffix) in enumerate(batch):\n",
    "                    if layer_name == \"model.embed_tokens\":\n",
    "                        batch[j] = (layer(prefix), layer(suffix))\n",
    "                    elif layer_name == \"model.norm\":\n",
    "                        # Only keep the last token at this point\n",
    "                        batch[j] = (None, layer(suffix[torch.arange(n_suffixes), suffix_eos[j]][:, None]))\n",
    "                    elif layer_name == \"value_head\":\n",
    "                        batch[j] = layer(suffix)[:, 0].mean(1).detach().cpu().numpy()\n",
    "                    else:\n",
    "                        # Run prefix\n",
    "                        len_p, len_s = prefix.shape[1], suffix.shape[1]\n",
    "                        new_prefix, (k_cache, v_cache) = layer(prefix, use_cache=True, attention_mask=attention_mask[:, :, -len_p:, -len_p:])\n",
    "                        \n",
    "                        # Run suffix\n",
    "                        pos = position_ids[:, len_p:len_p + len_s].expand(n_suffixes, -1)\n",
    "                        attn = attention_mask[:, :, -len_s:, -len_p - len_s:].expand(n_suffixes, -1, -1, -1)\n",
    "                        kv_cache = (k_cache.expand(n_suffixes, -1, -1, -1), v_cache.expand(n_suffixes, -1, -1, -1))\n",
    "                        new_suffix = layer(suffix, past_key_value=kv_cache, position_ids=pos, attention_mask=attn)[0]\n",
    "                        batch[j] = (new_prefix, new_suffix)\n",
    "\n",
    "                # Remove previous layer from memory (including buffers)\n",
    "                layer.to(\"meta\")\n",
    "                clean_memory() # proposed by CPMP\n",
    "\n",
    "        # Get scores\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-11T04:04:37.51561Z",
     "iopub.status.busy": "2023-10-11T04:04:37.514962Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run model on the 2 GPUs\n",
    "\n",
    "def get_tokens(row, tokenizer): \n",
    "        system_prefix = \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\nContext:\\n{context}\"\n",
    "        instruction = \"Your task is to analyze the question and answer below. If the answer is correct, respond yes, if it is not correct respond no. As a potential aid to your answer, background context from Wikipedia articles is at your disposal, even if they might not always be relevant.\"\n",
    "\n",
    "        # max length : MAX_LENGTH\n",
    "        prompt_suffix = [f\"{row[letter]}\\n\\n### Response:\\n\" for letter in \"ABCDE\"]\n",
    "        suffix = tokenizer(prompt_suffix, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=MAX_LENGTH, padding=True)[\"input_ids\"][:, 1:]\n",
    "\n",
    "        # max length : max(0, MAX_LENGTH - len(suffix))\n",
    "        prompt_question = f\"\\nQuestion: {row['prompt']}\\nProposed answer: \"\n",
    "        question = tokenizer(prompt_question, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=max(0, MAX_LENGTH - suffix.shape[1]))[\"input_ids\"][:, 1:]\n",
    "\n",
    "        # max length : min(MAX_CONTEXT, max(0, MAX_LENGTH - len(suffix) - len(question)))\n",
    "        prompt_context = system_prefix.format(instruction=instruction, context=row[\"context\"])\n",
    "        max_length = min(MAX_CONTEXT, max(0, MAX_LENGTH - question.shape[1] - suffix.shape[1]))\n",
    "        context = tokenizer(prompt_context, return_tensors=\"pt\", return_attention_mask=False, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "        prefix = torch.cat([context, question], dim=1)\n",
    "        return prefix, suffix\n",
    "\n",
    "def run_model(device, df, weights_loader):\n",
    "    model = ShardedLlama(checkpoint_path, weights_loader, device=device)\n",
    "    f = partial(get_tokens, tokenizer=model.tokenizer)\n",
    "    inputs = df.apply(f, axis=1).values\n",
    "    batches = np.array_split(inputs, N_BATCHES)\n",
    "    outputs = []\n",
    "    for i, batch in enumerate(batches):\n",
    "        outputs += model(batch)\n",
    "    return outputs\n",
    "\n",
    "# Run model\n",
    "if IS_TEST_SET:\n",
    "    devices = [f\"cuda:{i}\" for i in range(torch.cuda.device_count())]\n",
    "    weights_loader = WeightsLoader(checkpoint_path, devices)\n",
    "    f = partial(run_model, weights_loader=weights_loader) # added by treesky\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        outputs = list(executor.map(f, devices, np.array_split(df, 2)))\n",
    "        outputs = sum(outputs, [])\n",
    "        \n",
    "    # Save results\n",
    "    n = len(df)\n",
    "    for i, scores in enumerate(outputs):\n",
    "        top3 = np.argsort(scores)[::-1]\n",
    "        df.loc[i, \"prediction\"] = \" \".join([\"ABCDE\"[j] for j in top3])\n",
    "    \n",
    "    # Display performances if train set is used \n",
    "    if \"answer\" in df.columns:\n",
    "        for i in range(n):\n",
    "            df.loc[i, \"top_1\"] = df.loc[i, \"prediction\"][0]\n",
    "            df.loc[i, \"top_2\"] = df.loc[i, \"prediction\"][2]\n",
    "            df.loc[i, \"top_3\"] = df.loc[i, \"prediction\"][4]\n",
    "\n",
    "        top_i = [(df[f\"top_{i}\"] == df[\"answer\"]).sum() for i in [1, 2, 3]]\n",
    "        print(f\"top1 : {top_i[0]}/{n}, top2 : {top_i[1]}/{n}, top3 : {top_i[2]}/{n} (total={sum(top_i)} / {n})\")\n",
    "        print(f\"Accuracy: {100*top_i[0]/n:.1f}%, map3: {100*(top_i[0] + top_i[1]*1/2 + top_i[2]*1/3).sum()/n:.1f}%\")\n",
    "else:\n",
    "    df[\"prediction\"] = \"A B C\"\n",
    "\n",
    "df[[\"prediction\"]].to_csv(\"submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
